{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd with tensors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alelom/Notebooks/blob/master/Statistical%20Learning/Pytorch/Exercise%20Files/Autograd_with_tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPXHZpput5Xf",
        "colab_type": "text"
      },
      "source": [
        "# Autograd with tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-C6nLqAt98a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRSfe0Lut952",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor = torch.randn(4,3) # matrix from normal distribution"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-QcOYO1tkOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "db7a1fcd-db20-434b-f6e4-49649f6a4f6f"
      },
      "source": [
        "tensor.requires_grad_(True) # always calculate the gradient for this tensor, updating it every time it's modified, and store it in the same tensor object."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1698, -0.3622, -0.8116],\n",
              "        [-0.4120, -0.9719,  0.7049],\n",
              "        [-2.0413,  0.4612, -0.1069],\n",
              "        [-0.7725, -1.6714,  1.6946]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W74P-vq5Frhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2bea1be8-9118-4896-9803-7b562b1b4c92"
      },
      "source": [
        "print(tensor.grad_fn) # None, why?\n",
        "print(tensor.grad)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuWckF34t8X1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "97a0a3b7-6e89-4789-a434-1b4289b562e7"
      },
      "source": [
        "# If we create a new tensor based on the first, it will keep the requires_grad setting: the new gradient will already be there.\n",
        "y = torch.exp(tensor) # a new tensor y = e^tensor\n",
        "print(y) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.1851, 0.6961, 0.4442],\n",
            "        [0.6623, 0.3784, 2.0236],\n",
            "        [0.1299, 1.5861, 0.8986],\n",
            "        [0.4618, 0.1880, 5.4443]], grad_fn=<ExpBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkQCjuabt8Vh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "5cd08482-db84-4257-ab1c-86dac9cd2810"
      },
      "source": [
        "print(y.grad_fn)  # ExpBackward object\n",
        "print(y.grad)     # None\n",
        "# UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. \n",
        "# Its .grad attribute won't be populated during autograd.backward(). \n",
        "# If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor.\n",
        "# If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. \n",
        "# See github.com/pytorch/pytorch/pull/30531 for more information."
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ExpBackward object at 0x7f0092436cf8>\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvggJC5wt8Ti",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34ce6e2c-0c9b-49f7-8ba2-74ac42baa2f4"
      },
      "source": [
        "# operation to get a scalar value from the second tensor y\n",
        "scalarValue = y.mean()\n",
        "scalarValue"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1749, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEg2PQiXt8RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In order to get the gradient of the first tensor, we need to do a backward pass.\n",
        "scalarValue.backward() # backward pass from this last tensor to the very first one."
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZopoC8It8O7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "bf9adc3b-2b3f-4467-c38e-d25ef0d9002b"
      },
      "source": [
        "# now the first tensor has the gradient value:\n",
        "print(tensor.grad)    # this now has values.\n",
        "print(tensor.grad_fn) # still no gradient function."
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0988, 0.0580, 0.0370],\n",
            "        [0.0552, 0.0315, 0.1686],\n",
            "        [0.0108, 0.1322, 0.0749],\n",
            "        [0.0385, 0.0157, 0.4537]])\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ManI6K2rt8HD",
        "colab_type": "text"
      },
      "source": [
        "## Detaching a tensor from grad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbQpKI4it8EJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "0dfcbd96-5f2e-4a45-a5ce-9f10e1523fda"
      },
      "source": [
        "tensor.detach() # the gradient for this tensor will not be updated??"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1698, -0.3622, -0.8116],\n",
              "        [-0.4120, -0.9719,  0.7049],\n",
              "        [-2.0413,  0.4612, -0.1069],\n",
              "        [-0.7725, -1.6714,  1.6946]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq3KkSGvt8B_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1b23e2be-ab5b-4fda-f20a-788fc8ceae2a"
      },
      "source": [
        "# When you want to evaluate a model, sometimes you might want to prevent tracking history;\n",
        "# this because that model might have trainable parameters. \n",
        "# I.e. you have requires_grad set to True for the model,\n",
        "# but for whatever reason you don't want the gradient calculated, and simply want the output.\n",
        "\n",
        "print(scalarValue.requires_grad) # this is True\n",
        "\n",
        "with torch.no_grad():\n",
        "  scalarValue = y.mean()\n",
        "\n",
        "print(scalarValue) # this does not have 'grad_fn=<MeanBackward0>' anymore\n",
        "print(scalarValue.requires_grad) # this is now False."
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor(1.1749)\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}